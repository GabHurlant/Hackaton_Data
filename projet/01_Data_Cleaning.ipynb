{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4158c98e",
      "metadata": {
        "id": "4158c98e"
      },
      "source": [
        "# üßπ Data Cleaning - Making Sense of French Health Data\n",
        "\n",
        "**For Decision-Makers**: Think of this as organizing a messy filing cabinet. We're taking scattered, inconsistent data and turning it into a well-organized database you can trust. Without this step, any analysis would be unreliable.\n",
        "\n",
        "**Goal**: Transform messy raw data into clean, analysis-ready datasets.\n",
        "\n",
        "**What we'll do**:\n",
        "1. Fix encoding issues (French characters like √©, √†, √ß)\n",
        "2. Standardize date formats (so Jan 2020 always means the same thing)\n",
        "3. Clean region names (align to 13 official French regions)\n",
        "4. Handle missing values intelligently (filling gaps the right way)\n",
        "5. Create one unified dataset at regional level\n",
        "\n",
        "**Why regional level?**\n",
        "- ‚ùå National is too aggregated (hides important patterns)\n",
        "- ‚ùå Departmental is too granular (sparse data, hard to predict)\n",
        "- ‚úÖ Regional is the sweet spot for forecasting (13 regions = manageable + meaningful)\n",
        "\n",
        "## üéØ Business Value:\n",
        "Clean data means:\n",
        "- Accurate forecasts (garbage in = garbage out!)\n",
        "- Reliable recommendations (decision-makers can trust the results)\n",
        "- Faster analysis (no time wasted debugging data issues)\n",
        "- Reproducible process (can be rerun with new data)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cfec639f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfec639f",
        "outputId": "aa1df66a-47df-4a8a-b1b1-d9bcaf098b53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Libraries loaded\n",
            "üìÖ 2025-10-21 15:06\n",
            "üñ•Ô∏è Environment: Local\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "import sys\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Detect environment (check if running in Google Colab)\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Mount Google Drive if in Colab\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive mounted\")\n",
        "\n",
        "print(\"‚úÖ Libraries loaded\")\n",
        "print(f\"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
        "print(f\"üñ•Ô∏è Environment: {'Google Colab' if IN_COLAB else 'Local'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7ae922bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ae922bc",
        "outputId": "27a9cf98-e008-434b-9cf2-faef158d65dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Input: c:\\Users\\gabin\\Desktop\\epitech\\hackaton-sante\\projet\\DATASET\n",
            "üìÇ Output: c:\\Users\\gabin\\Desktop\\epitech\\hackaton-sante\\projet\\data\\processed\n",
            "üìÇ Data exists: True\n"
          ]
        }
      ],
      "source": [
        "# Paths (works both locally and in Colab)\n",
        "if IN_COLAB:\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/HACKATHON_DATALAB')\n",
        "else:\n",
        "    BASE_PATH = Path.cwd()\n",
        "\n",
        "DATA_PATH = BASE_PATH / 'DATASET'\n",
        "OUTPUT_PATH = BASE_PATH / 'data' / 'processed'\n",
        "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"üìÇ Input: {DATA_PATH}\")\n",
        "print(f\"üìÇ Output: {OUTPUT_PATH}\")\n",
        "print(f\"üìÇ Data exists: {DATA_PATH.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41c0d3ce",
      "metadata": {
        "id": "41c0d3ce"
      },
      "source": [
        "---\n",
        "\n",
        "## üõ†Ô∏è Helper Functions\n",
        "\n",
        "These are the workhorses that do the actual cleaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fe31d98b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe31d98b",
        "outputId": "b68aaac0-d814-46d2-9b7e-7db7cef38f2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Helper functions defined (with enhanced validation)\n"
          ]
        }
      ],
      "source": [
        "def load_csv_robust(filepath, encodings=['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']):\n",
        "    \"\"\"\n",
        "    Load CSV with multiple encoding attempts.\n",
        "    French data is notorious for encoding issues.\n",
        "    \"\"\"\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            df = pd.read_csv(filepath, encoding=encoding, low_memory=False)\n",
        "            print(f\"‚úÖ Loaded with {encoding}: {filepath.name}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    print(f\"‚ùå Could not load: {filepath.name}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def parse_dates_flexible(df, date_columns=None):\n",
        "    \"\"\"\n",
        "    Try to parse date columns with multiple formats.\n",
        "    French dates can be: dd/mm/yyyy, yyyy-mm-dd, or text like 'Semaine 2024-01'\n",
        "    \"\"\"\n",
        "    if date_columns is None:\n",
        "        # Auto-detect date columns\n",
        "        date_keywords = ['date', 'semaine', 'week', 'annee', 'year', 'periode', 'jour']\n",
        "        date_columns = [col for col in df.columns if any(kw in col.lower() for kw in date_keywords)]\n",
        "\n",
        "    for col in date_columns:\n",
        "        if col in df.columns:\n",
        "            try:\n",
        "                # Try standard parsing first\n",
        "                df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)\n",
        "\n",
        "                # If most values parsed successfully, keep it\n",
        "                if df[col].notna().sum() / len(df) > 0.5:\n",
        "                    print(f\"   ‚úÖ Parsed date column: {col}\")\n",
        "                    # Create year and month for aggregation\n",
        "                    df[f'{col}_year'] = df[col].dt.year\n",
        "                    df[f'{col}_month'] = df[col].dt.month\n",
        "                    df[f'{col}_week'] = df[col].dt.isocalendar().week\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ö†Ô∏è Could not parse {col}: {e}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def standardize_region_names(df, region_col=None):\n",
        "    \"\"\"\n",
        "    Clean up region names to match official 13 French regions.\n",
        "    \"\"\"\n",
        "    # Official 13 French regions (post-2016 reform)\n",
        "    official_regions = [\n",
        "        'Auvergne-Rh√¥ne-Alpes',\n",
        "        'Bourgogne-Franche-Comt√©',\n",
        "        'Bretagne',\n",
        "        'Centre-Val de Loire',\n",
        "        'Corse',\n",
        "        'Grand Est',\n",
        "        'Hauts-de-France',\n",
        "        '√éle-de-France',\n",
        "        'Normandie',\n",
        "        'Nouvelle-Aquitaine',\n",
        "        'Occitanie',\n",
        "        'Pays de la Loire',\n",
        "        \"Provence-Alpes-C√¥te d'Azur\"\n",
        "    ]\n",
        "\n",
        "    # Auto-detect region column\n",
        "    if region_col is None:\n",
        "        region_keywords = ['region', 'r√©gion', 'territoire']\n",
        "        for col in df.columns:\n",
        "            if any(kw in col.lower() for kw in region_keywords) and 'code' not in col.lower():\n",
        "                region_col = col\n",
        "                break\n",
        "\n",
        "    if region_col and region_col in df.columns:\n",
        "        # Clean up common variations\n",
        "        df[region_col] = df[region_col].astype(str).str.strip()\n",
        "\n",
        "        # Common replacements\n",
        "        replacements = {\n",
        "            'Ile-de-France': '√éle-de-France',\n",
        "            'Ile de France': '√éle-de-France',\n",
        "            'PACA': \"Provence-Alpes-C√¥te d'Azur\",\n",
        "            'Provence-Alpes-Cote d\\'Azur': \"Provence-Alpes-C√¥te d'Azur\",\n",
        "            'Auvergne-Rhone-Alpes': 'Auvergne-Rh√¥ne-Alpes',\n",
        "            'Bourgogne-Franche-Comte': 'Bourgogne-Franche-Comt√©'\n",
        "        }\n",
        "\n",
        "        df[region_col] = df[region_col].replace(replacements)\n",
        "\n",
        "        # Show unique regions found\n",
        "        unique_regions = df[region_col].unique()\n",
        "        print(f\"   ‚úÖ Found {len(unique_regions)} unique regions\")\n",
        "\n",
        "        # Check for unmapped regions\n",
        "        unmapped = set(unique_regions) - set(official_regions) - {'nan', 'None', ''}\n",
        "        if unmapped:\n",
        "            print(f\"   ‚ö†Ô∏è Unmapped regions: {unmapped}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def handle_missing_values(df, strategy='report'):\n",
        "    \"\"\"\n",
        "    Handle missing values intelligently with comprehensive reporting.\n",
        "    Strategy:\n",
        "    - 'report': Just show what's missing\n",
        "    - 'drop': Drop rows with any missing values\n",
        "    - 'fill': Fill with appropriate defaults\n",
        "    \"\"\"\n",
        "    missing = df.isnull().sum()\n",
        "    missing_pct = (missing / len(df)) * 100\n",
        "\n",
        "    if missing.any():\n",
        "        print(f\"\\n   ‚ö†Ô∏è Missing values:\")\n",
        "        for col in missing[missing > 0].index:\n",
        "            print(f\"      {col}: {missing[col]:,} ({missing_pct[col]:.1f}%)\")\n",
        "\n",
        "        if strategy == 'drop':\n",
        "            original_len = len(df)\n",
        "            df = df.dropna()\n",
        "            print(f\"   ‚úÖ Dropped {original_len - len(df):,} rows with missing values\")\n",
        "\n",
        "        elif strategy == 'fill':\n",
        "            # Fill numeric columns with 0, object columns with 'Unknown'\n",
        "            for col in df.columns:\n",
        "                if df[col].dtype in ['float64', 'int64']:\n",
        "                    df[col].fillna(0, inplace=True)\n",
        "                else:\n",
        "                    df[col].fillna('Unknown', inplace=True)\n",
        "            print(f\"   ‚úÖ Filled missing values\")\n",
        "    else:\n",
        "        print(f\"   ‚úÖ No missing values\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def validate_data_quality(df, name=\"Dataset\"):\n",
        "    \"\"\"\n",
        "    Comprehensive data quality validation checks.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üìã DATA QUALITY VALIDATION: {name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    issues = []\n",
        "    \n",
        "    # 1. Check for duplicates\n",
        "    duplicates = df.duplicated().sum()\n",
        "    if duplicates > 0:\n",
        "        print(f\"‚ö†Ô∏è Found {duplicates} duplicate rows ({duplicates/len(df)*100:.1f}%)\")\n",
        "        issues.append(f\"Duplicates: {duplicates}\")\n",
        "    else:\n",
        "        print(f\"‚úÖ No duplicate rows\")\n",
        "    \n",
        "    # 2. Check numeric columns for negative values where inappropriate\n",
        "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    for col in numeric_cols:\n",
        "        if 'taux' in col.lower() or 'nombre' in col.lower() or 'passage' in col.lower():\n",
        "            negative_count = (df[col] < 0).sum()\n",
        "            if negative_count > 0:\n",
        "                print(f\"‚ö†Ô∏è {col}: {negative_count} negative values (should be positive)\")\n",
        "                issues.append(f\"{col}: negative values\")\n",
        "            else:\n",
        "                print(f\"‚úÖ {col}: No negative values\")\n",
        "    \n",
        "    # 3. Check for outliers (values > 3 standard deviations)\n",
        "    for col in numeric_cols:\n",
        "        if df[col].std() > 0:  # Avoid division by zero\n",
        "            mean = df[col].mean()\n",
        "            std = df[col].std()\n",
        "            outliers = ((df[col] < mean - 3*std) | (df[col] > mean + 3*std)).sum()\n",
        "            if outliers > 0:\n",
        "                pct = outliers / len(df) * 100\n",
        "                if pct > 5:  # Only warn if > 5% are outliers\n",
        "                    print(f\"‚ö†Ô∏è {col}: {outliers} outliers ({pct:.1f}%) - may need investigation\")\n",
        "                    issues.append(f\"{col}: {outliers} outliers\")\n",
        "    \n",
        "    # 4. Check date column consistency\n",
        "    if 'date' in df.columns:\n",
        "        date_col = df['date']\n",
        "        if pd.api.types.is_datetime64_any_dtype(date_col):\n",
        "            # Check for future dates\n",
        "            today = pd.Timestamp.now()\n",
        "            future_dates = (date_col > today).sum()\n",
        "            if future_dates > 0:\n",
        "                print(f\"‚ö†Ô∏è Date column: {future_dates} future dates (suspicious)\")\n",
        "                issues.append(f\"Future dates: {future_dates}\")\n",
        "            \n",
        "            # Check for very old dates (before 2010)\n",
        "            very_old = (date_col < pd.Timestamp('2010-01-01')).sum()\n",
        "            if very_old > 0:\n",
        "                print(f\"‚ö†Ô∏è Date column: {very_old} dates before 2010 (check if valid)\")\n",
        "                issues.append(f\"Very old dates: {very_old}\")\n",
        "            \n",
        "            # Check date range\n",
        "            date_range = (date_col.max() - date_col.min()).days\n",
        "            print(f\"‚úÖ Date range: {date_col.min().date()} to {date_col.max().date()} ({date_range} days)\")\n",
        "    \n",
        "    # 5. Check regional coverage\n",
        "    if 'region' in df.columns:\n",
        "        unique_regions = df['region'].nunique()\n",
        "        expected_regions = 13  # France has 13 regions\n",
        "        print(f\"üìä Regions found: {unique_regions}\")\n",
        "        if unique_regions < expected_regions:\n",
        "            print(f\"‚ö†Ô∏è Expected {expected_regions} regions, found {unique_regions}\")\n",
        "            issues.append(f\"Missing regions: {expected_regions - unique_regions}\")\n",
        "    \n",
        "    # Summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    if issues:\n",
        "        print(f\"‚ö†Ô∏è Found {len(issues)} data quality issues:\")\n",
        "        for issue in issues:\n",
        "            print(f\"   - {issue}\")\n",
        "        print(f\"\\nüí° Review these issues before proceeding with analysis\")\n",
        "    else:\n",
        "        print(f\"‚úÖ All data quality checks passed!\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    return issues\n",
        "\n",
        "print(\"‚úÖ Helper functions defined (with enhanced validation)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef28a0c2",
      "metadata": {
        "id": "ef28a0c2"
      },
      "source": [
        "---\n",
        "\n",
        "## üìä Load & Clean: Vaccination Coverage Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "41e0813a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41e0813a",
        "outputId": "5e4f881b-79d8-4b81-b418-cb5455c8ded9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä LOADING VACCINATION COVERAGE DATA\n",
            "============================================================\n",
            "‚úÖ Loaded with utf-8: couvertures-vaccinales-des-adolescents-et-adultes-depuis-2011-region.csv\n",
            "\n",
            "üìè Original shape: (238, 19)\n",
            "üìã Columns: ['Ann√©e', 'R√©gion Code', 'R√©gion', 'HPV filles 1 dose √† 15 ans', 'HPV filles 2 doses √† 16 ans', 'HPV gar√ßons 1 dose √† 15 ans', 'HPV gar√ßons 2 doses √† 16 ans', 'M√©ningocoque C 10-14 ans', 'M√©ningocoque C 15-19 ans', 'M√©ningocoque C 20-24 ans', 'Grippe moins de 65 ans √† risque', 'Grippe 65 ans et plus', 'Grippe 65-74 ans', 'Grippe 75 ans et plus', 'Covid-19 65 ans et plus', 'Grippe r√©sidents en Ehpad', 'Grippe professionnels en Ehpad', 'Covid-19 r√©sidents en Ehpad', 'Covid-19 professionnels en Ehpad']\n",
            "   ‚úÖ Found 17 unique regions\n",
            "   ‚ö†Ô∏è Unmapped regions: {'Auvergne et Rh√¥ne-Alpes', 'Nouvelle Aquitaine', 'Bourgogne et Franche-Comt√©', 'Martinique', 'Guyane', 'R√©union', 'Guadeloupe'}\n",
            "\n",
            "   ‚ö†Ô∏è Missing values:\n",
            "      HPV filles 1 dose √† 15 ans: 20 (8.4%)\n",
            "      HPV filles 2 doses √† 16 ans: 20 (8.4%)\n",
            "      HPV gar√ßons 1 dose √† 15 ans: 187 (78.6%)\n",
            "      HPV gar√ßons 2 doses √† 16 ans: 187 (78.6%)\n",
            "      M√©ningocoque C 10-14 ans: 59 (24.8%)\n",
            "      M√©ningocoque C 15-19 ans: 59 (24.8%)\n",
            "      M√©ningocoque C 20-24 ans: 221 (92.9%)\n",
            "      Grippe moins de 65 ans √† risque: 97 (40.8%)\n",
            "      Grippe 65 ans et plus: 97 (40.8%)\n",
            "      Grippe 65-74 ans: 190 (79.8%)\n",
            "      Grippe 75 ans et plus: 190 (79.8%)\n",
            "      Covid-19 65 ans et plus: 221 (92.9%)\n",
            "      Grippe r√©sidents en Ehpad: 238 (100.0%)\n",
            "      Grippe professionnels en Ehpad: 238 (100.0%)\n",
            "      Covid-19 r√©sidents en Ehpad: 238 (100.0%)\n",
            "      Covid-19 professionnels en Ehpad: 238 (100.0%)\n",
            "\n",
            "‚úÖ Cleaned vaccination data: (238, 19)\n",
            "\n",
            "üëÄ Sample:\n",
            "   Ann√©e  R√©gion Code               R√©gion  HPV filles 1 dose √† 15 ans  \\\n",
            "0   2011           24  Centre-Val de Loire                        32.0   \n",
            "1   2011           44            Grand Est                        35.6   \n",
            "2   2011           32      Hauts-de-France                        38.7   \n",
            "\n",
            "   HPV filles 2 doses √† 16 ans  HPV gar√ßons 1 dose √† 15 ans  \\\n",
            "0                         28.6                          NaN   \n",
            "1                         30.0                          NaN   \n",
            "2                         35.2                          NaN   \n",
            "\n",
            "   HPV gar√ßons 2 doses √† 16 ans  M√©ningocoque C 10-14 ans  \\\n",
            "0                           NaN                       NaN   \n",
            "1                           NaN                       NaN   \n",
            "2                           NaN                       NaN   \n",
            "\n",
            "   M√©ningocoque C 15-19 ans  M√©ningocoque C 20-24 ans  \\\n",
            "0                       NaN                       NaN   \n",
            "1                       NaN                       NaN   \n",
            "2                       NaN                       NaN   \n",
            "\n",
            "   Grippe moins de 65 ans √† risque  Grippe 65 ans et plus  Grippe 65-74 ans  \\\n",
            "0                              NaN                    NaN               NaN   \n",
            "1                              NaN                    NaN               NaN   \n",
            "2                              NaN                    NaN               NaN   \n",
            "\n",
            "   Grippe 75 ans et plus  Covid-19 65 ans et plus  Grippe r√©sidents en Ehpad  \\\n",
            "0                    NaN                      NaN                        NaN   \n",
            "1                    NaN                      NaN                        NaN   \n",
            "2                    NaN                      NaN                        NaN   \n",
            "\n",
            "   Grippe professionnels en Ehpad  Covid-19 r√©sidents en Ehpad  \\\n",
            "0                             NaN                          NaN   \n",
            "1                             NaN                          NaN   \n",
            "2                             NaN                          NaN   \n",
            "\n",
            "   Covid-19 professionnels en Ehpad  \n",
            "0                               NaN  \n",
            "1                               NaN  \n",
            "2                               NaN  \n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüìä LOADING VACCINATION COVERAGE DATA\\n\" + \"=\"*60)\n",
        "\n",
        "# Regional vaccination coverage (our focus)\n",
        "vax_path = DATA_PATH / 'Couvertures-vaccinales-des-adolescents-et-adultes' / 'Donn√©es-r√©gionales' / 'couvertures-vaccinales-des-adolescents-et-adultes-depuis-2011-region.csv'\n",
        "\n",
        "if vax_path.exists():\n",
        "    df_vaccination = load_csv_robust(vax_path)\n",
        "\n",
        "    if df_vaccination is not None:\n",
        "        print(f\"\\nüìè Original shape: {df_vaccination.shape}\")\n",
        "        print(f\"üìã Columns: {list(df_vaccination.columns)}\")\n",
        "\n",
        "        # Parse dates\n",
        "        df_vaccination = parse_dates_flexible(df_vaccination)\n",
        "\n",
        "        # Standardize regions\n",
        "        df_vaccination = standardize_region_names(df_vaccination)\n",
        "\n",
        "        # Check missing values\n",
        "        df_vaccination = handle_missing_values(df_vaccination, strategy='report')\n",
        "\n",
        "        print(f\"\\n‚úÖ Cleaned vaccination data: {df_vaccination.shape}\")\n",
        "        print(f\"\\nüëÄ Sample:\")\n",
        "        print(df_vaccination.head(3))\n",
        "else:\n",
        "    print(f\"‚ùå File not found: {vax_path}\")\n",
        "    df_vaccination = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a3d7615",
      "metadata": {
        "id": "0a3d7615"
      },
      "source": [
        "---\n",
        "\n",
        "## üè• Load & Clean: Emergency Room Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "14c7396a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14c7396a",
        "outputId": "074d5bb7-9647-470b-ca0d-c49227e45d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üè• LOADING EMERGENCY ROOM DATA\n",
            "============================================================\n",
            "‚úÖ Loaded with utf-8: grippe-passages-urgences-et-actes-sos-medecin_reg.csv\n",
            "\n",
            "üìè Original shape: (27180, 8)\n",
            "üìã Columns: ['1er jour de la semaine', 'Semaine', 'R√©gion Code', 'R√©gion', \"Classe d'√¢ge\", 'Taux de passages aux urgences pour grippe', \"Taux d'hospitalisations apr√®s passages aux urgences pour grippe\", \"Taux d'actes m√©dicaux SOS m√©decins pour grippe\"]\n",
            "   ‚úÖ Parsed date column: 1er jour de la semaine\n",
            "   ‚úÖ Found 18 unique regions\n",
            "   ‚ö†Ô∏è Unmapped regions: {'Mayotte', 'Auvergne et Rh√¥ne-Alpes', 'Nouvelle Aquitaine', 'Bourgogne et Franche-Comt√©', 'Martinique', 'Guyane', 'R√©union', 'Guadeloupe'}\n",
            "\n",
            "   ‚ö†Ô∏è Missing values:\n",
            "      Semaine: 27,180 (100.0%)\n",
            "      Taux de passages aux urgences pour grippe: 890 (3.3%)\n",
            "      Taux d'hospitalisations apr√®s passages aux urgences pour grippe: 897 (3.3%)\n",
            "      Taux d'actes m√©dicaux SOS m√©decins pour grippe: 6,045 (22.2%)\n",
            "\n",
            "‚úÖ Cleaned emergency data: (27180, 11)\n",
            "\n",
            "üëÄ Sample:\n",
            "  1er jour de la semaine Semaine  R√©gion Code   R√©gion    Classe d'√¢ge  \\\n",
            "0             2023-02-20     NaT            6  Mayotte       00-04 ans   \n",
            "1             2023-02-20     NaT            6  Mayotte       15-64 ans   \n",
            "2             2023-02-20     NaT            6  Mayotte  65 ans ou plus   \n",
            "\n",
            "   Taux de passages aux urgences pour grippe  \\\n",
            "0                                 383.141762   \n",
            "1                                 728.597450   \n",
            "2                                2127.659574   \n",
            "\n",
            "   Taux d'hospitalisations apr√®s passages aux urgences pour grippe  \\\n",
            "0                                             1562.5                 \n",
            "1                                                0.0                 \n",
            "2                                                0.0                 \n",
            "\n",
            "   Taux d'actes m√©dicaux SOS m√©decins pour grippe  \\\n",
            "0                                             NaN   \n",
            "1                                             NaN   \n",
            "2                                             NaN   \n",
            "\n",
            "   1er jour de la semaine_year  1er jour de la semaine_month  \\\n",
            "0                         2023                             2   \n",
            "1                         2023                             2   \n",
            "2                         2023                             2   \n",
            "\n",
            "   1er jour de la semaine_week  \n",
            "0                            8  \n",
            "1                            8  \n",
            "2                            8  \n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüè• LOADING EMERGENCY ROOM DATA\\n\" + \"=\"*60)\n",
        "\n",
        "# Regional emergency data (weekly time series)\n",
        "emerg_path = DATA_PATH / 'Passages-aux-urgences-et-Actes-SOS-M√©decins' / 'Donn√©es-r√©gionales' / 'grippe-passages-urgences-et-actes-sos-medecin_reg.csv'\n",
        "\n",
        "if emerg_path.exists():\n",
        "    df_emergency = load_csv_robust(emerg_path)\n",
        "\n",
        "    if df_emergency is not None:\n",
        "        print(f\"\\nüìè Original shape: {df_emergency.shape}\")\n",
        "        print(f\"üìã Columns: {list(df_emergency.columns)}\")\n",
        "\n",
        "        # Parse dates\n",
        "        df_emergency = parse_dates_flexible(df_emergency)\n",
        "\n",
        "        # Standardize regions\n",
        "        df_emergency = standardize_region_names(df_emergency)\n",
        "\n",
        "        # Check missing values\n",
        "        df_emergency = handle_missing_values(df_emergency, strategy='report')\n",
        "\n",
        "        print(f\"\\n‚úÖ Cleaned emergency data: {df_emergency.shape}\")\n",
        "        print(f\"\\nüëÄ Sample:\")\n",
        "        print(df_emergency.head(3))\n",
        "else:\n",
        "    print(f\"‚ùå File not found: {emerg_path}\")\n",
        "    df_emergency = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72891f2a",
      "metadata": {
        "id": "72891f2a"
      },
      "source": [
        "---\n",
        "\n",
        "## üíâ Load & Clean: Flu Campaign Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "73fa6e7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73fa6e7b",
        "outputId": "5337c402-f0f6-42e2-c5da-4e70f0f2a134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíâ LOADING FLU CAMPAIGN DATA\n",
            "============================================================\n",
            "\n",
            "üìÖ Processing 2021-2022...\n",
            "‚úÖ Loaded with utf-8: campagne-2021.csv\n",
            "   ‚úÖ Parsed date column: date\n",
            "   campagne: (5, 8)\n",
            "‚úÖ Loaded with utf-8: couverture-2021.csv\n",
            "   ‚úÖ Found 13 unique regions\n",
            "   ‚ö†Ô∏è Unmapped regions: {'11 - ILE-DE-France', '53 - BRETAGNE', '44 - GRAND-EST', '24 - CENTRE-VAL-DE-LOIRE', '76 - OCCITANIE', '27 - BOURGOGNE-FRANCHE-COMTE', '28 - NORMANDIE', '32 - HAUTS-DE-France', '84 - AUVERGNE-RHONE-ALPES', \"93 - PROVENCE-ALPES-COTES-D'AZUR\", '52 - PAYS-DE-LA-LOIRE', '94 - CORSE', '75 - NOUVELLE-AQUITAINE'}\n",
            "   couverture: (52, 5)\n",
            "‚úÖ Loaded with utf-8: doses-actes-2021.csv\n",
            "   ‚úÖ Parsed date column: date\n",
            "   ‚úÖ Parsed date column: jour\n",
            "   doses-actes: (1076, 12)\n",
            "\n",
            "üìÖ Processing 2022-2023...\n",
            "‚úÖ Loaded with utf-8: campagne-2022.csv\n",
            "   ‚úÖ Parsed date column: date\n",
            "   campagne: (5, 8)\n",
            "‚úÖ Loaded with utf-8: couverture-2022.csv\n",
            "   ‚úÖ Found 13 unique regions\n",
            "   ‚ö†Ô∏è Unmapped regions: {'11 - ILE-DE-France', '53 - BRETAGNE', '44 - GRAND-EST', '24 - CENTRE-VAL-DE-LOIRE', '76 - OCCITANIE', '27 - BOURGOGNE-FRANCHE-COMTE', '28 - NORMANDIE', '32 - HAUTS-DE-France', '84 - AUVERGNE-RHONE-ALPES', \"93 - PROVENCE-ALPES-COTES-D'AZUR\", '52 - PAYS-DE-LA-LOIRE', '94 - CORSE', '75 - NOUVELLE-AQUITAINE'}\n",
            "   couverture: (52, 5)\n",
            "‚úÖ Loaded with utf-8: doses-actes-2022.csv\n",
            "   ‚úÖ Parsed date column: date\n",
            "   ‚úÖ Parsed date column: jour\n",
            "   doses-actes: (1056, 12)\n",
            "\n",
            "üìÖ Processing 2023-2024...\n",
            "‚úÖ Loaded with utf-8: campagne-2023.csv\n",
            "   ‚úÖ Parsed date column: date\n",
            "   campagne: (5, 8)\n",
            "‚úÖ Loaded with utf-8: couverture-2023.csv\n",
            "   ‚úÖ Found 13 unique regions\n",
            "   ‚ö†Ô∏è Unmapped regions: {'11 - ILE-DE-France', '53 - BRETAGNE', '44 - GRAND-EST', '24 - CENTRE-VAL-DE-LOIRE', '76 - OCCITANIE', '27 - BOURGOGNE-FRANCHE-COMTE', '28 - NORMANDIE', '32 - HAUTS-DE-France', '84 - AUVERGNE-RHONE-ALPES', \"93 - PROVENCE-ALPES-COTES-D'AZUR\", '52 - PAYS-DE-LA-LOIRE', '94 - CORSE', '75 - NOUVELLE-AQUITAINE'}\n",
            "   couverture: (52, 5)\n",
            "‚úÖ Loaded with utf-8: doses-actes-2023.csv\n",
            "   ‚úÖ Parsed date column: date\n",
            "   ‚úÖ Parsed date column: jour\n",
            "   doses-actes: (1076, 12)\n",
            "\n",
            "üìÖ Processing 2024-2025...\n",
            "‚úÖ Loaded with utf-8: campagne-2024.csv\n",
            "   ‚úÖ Parsed date column: date\n",
            "   campagne: (5, 8)\n",
            "‚úÖ Loaded with utf-8: couverture-2024.csv\n",
            "   ‚úÖ Found 13 unique regions\n",
            "   ‚ö†Ô∏è Unmapped regions: {'11 - ILE-DE-France', '53 - BRETAGNE', '44 - GRAND-EST', '24 - CENTRE-VAL-DE-LOIRE', '76 - OCCITANIE', '27 - BOURGOGNE-FRANCHE-COMTE', '28 - NORMANDIE', '32 - HAUTS-DE-France', '84 - AUVERGNE-RHONE-ALPES', \"93 - PROVENCE-ALPES-COTES-D'AZUR\", '52 - PAYS-DE-LA-LOIRE', '94 - CORSE', '75 - NOUVELLE-AQUITAINE'}\n",
            "   couverture: (52, 5)\n",
            "‚úÖ Loaded with utf-8: doses-actes-2024.csv\n",
            "   ‚úÖ Parsed date column: date\n",
            "   ‚úÖ Parsed date column: jour\n",
            "   doses-actes: (964, 12)\n",
            "\n",
            "‚úÖ Loaded 4 campaign years\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüíâ LOADING FLU CAMPAIGN DATA\\n\" + \"=\"*60)\n",
        "\n",
        "# Load all campaign years\n",
        "flu_campaigns = {}\n",
        "flu_base = DATA_PATH / 'Vaccination-Grippe'\n",
        "\n",
        "for year_folder in sorted(flu_base.glob('Vaccination-Grippe-*')):\n",
        "    year = year_folder.name.replace('Vaccination-Grippe-', '')\n",
        "    print(f\"\\nüìÖ Processing {year}...\")\n",
        "\n",
        "    year_data = {}\n",
        "\n",
        "    for csv_file in sorted(year_folder.glob('*.csv')):\n",
        "        file_type = csv_file.stem.rsplit('-', 1)[0]  # Get 'campagne', 'couverture', 'doses-actes'\n",
        "\n",
        "        df = load_csv_robust(csv_file)\n",
        "        if df is not None:\n",
        "            df = parse_dates_flexible(df)\n",
        "            df = standardize_region_names(df)\n",
        "            year_data[file_type] = df\n",
        "            print(f\"   {file_type}: {df.shape}\")\n",
        "\n",
        "    if year_data:\n",
        "        flu_campaigns[year] = year_data\n",
        "\n",
        "print(f\"\\n‚úÖ Loaded {len(flu_campaigns)} campaign years\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "222bc106",
      "metadata": {
        "id": "222bc106"
      },
      "source": [
        "---\n",
        "\n",
        "## üîó Create Unified Regional Dataset\n",
        "\n",
        "**Goal**: Merge all data sources at the regional-weekly level.\n",
        "\n",
        "This will be our main dataset for forecasting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "236c6ca4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "236c6ca4",
        "outputId": "1c96d0e5-4b67-4e6d-e7f4-3cfdf2659629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîó CREATING UNIFIED DATASET\n",
            "============================================================\n",
            "Potential date columns found: ['1er jour de la semaine', 'Semaine', '1er jour de la semaine_year', '1er jour de la semaine_month', '1er jour de la semaine_week']\n",
            "Potential region columns found: ['R√©gion']\n",
            "üìÖ Using date column: 1er jour de la semaine\n",
            "üó∫Ô∏è Using region column: R√©gion\n",
            "\n",
            "‚úÖ Base dataset created: (27180, 11)\n",
            "üìÖ Date range: 2019-12-30 00:00:00 to 2025-10-06 00:00:00\n",
            "üó∫Ô∏è Regions: 18 unique regions\n",
            "   Regions: ['Auvergne et Rh√¥ne-Alpes', 'Bourgogne et Franche-Comt√©', 'Bretagne', 'Centre-Val de Loire', 'Corse', 'Grand Est', 'Guadeloupe', 'Guyane', 'Hauts-de-France', 'Martinique', 'Mayotte', 'Normandie', 'Nouvelle Aquitaine', 'Occitanie', 'Pays de la Loire', \"Provence-Alpes-C√¥te d'Azur\", 'R√©union', '√éle-de-France']\n",
            "üìä Total weeks: 302\n",
            "üìä Data completeness: 500.0%\n",
            "\n",
            "üëÄ Sample of unified dataset:\n",
            "        date Semaine  R√©gion Code                      region    Classe d'√¢ge  \\\n",
            "0 2019-12-30     NaT           84     Auvergne et Rh√¥ne-Alpes       05-14 ans   \n",
            "1 2019-12-30     NaT           84     Auvergne et Rh√¥ne-Alpes       Tous √¢ges   \n",
            "2 2019-12-30     NaT           84     Auvergne et Rh√¥ne-Alpes       00-04 ans   \n",
            "3 2019-12-30     NaT           84     Auvergne et Rh√¥ne-Alpes       15-64 ans   \n",
            "4 2019-12-30     NaT           84     Auvergne et Rh√¥ne-Alpes  65 ans ou plus   \n",
            "5 2019-12-30     NaT           27  Bourgogne et Franche-Comt√©       15-64 ans   \n",
            "6 2019-12-30     NaT           27  Bourgogne et Franche-Comt√©  65 ans ou plus   \n",
            "7 2019-12-30     NaT           27  Bourgogne et Franche-Comt√©       00-04 ans   \n",
            "8 2019-12-30     NaT           27  Bourgogne et Franche-Comt√©       05-14 ans   \n",
            "9 2019-12-30     NaT           27  Bourgogne et Franche-Comt√©       Tous √¢ges   \n",
            "\n",
            "   Taux de passages aux urgences pour grippe  \\\n",
            "0                                 650.142219   \n",
            "1                                 526.218269   \n",
            "2                                 784.199826   \n",
            "3                                 519.702651   \n",
            "4                                 340.193911   \n",
            "5                                 706.531105   \n",
            "6                                 100.738751   \n",
            "7                                 926.705981   \n",
            "8                                 495.662949   \n",
            "9                                 547.512992   \n",
            "\n",
            "   Taux d'hospitalisations apr√®s passages aux urgences pour grippe  \\\n",
            "0                                           0.000000                 \n",
            "1                                         308.344575                 \n",
            "2                                         621.118012                 \n",
            "3                                          51.867220                 \n",
            "4                                         471.512770                 \n",
            "5                                           0.000000                 \n",
            "6                                          76.628352                 \n",
            "7                                        1369.863014                 \n",
            "8                                           0.000000                 \n",
            "9                                         126.262626                 \n",
            "\n",
            "   Taux d'actes m√©dicaux SOS m√©decins pour grippe  \\\n",
            "0                                     2995.008320   \n",
            "1                                     3097.696585   \n",
            "2                                      564.334086   \n",
            "3                                     4415.440156   \n",
            "4                                     1000.834028   \n",
            "5                                     6311.360449   \n",
            "6                                     2564.102564   \n",
            "7                                     2727.272727   \n",
            "8                                     5220.883534   \n",
            "9                                     5079.100749   \n",
            "\n",
            "   1er jour de la semaine_year  1er jour de la semaine_month  \\\n",
            "0                         2019                            12   \n",
            "1                         2019                            12   \n",
            "2                         2019                            12   \n",
            "3                         2019                            12   \n",
            "4                         2019                            12   \n",
            "5                         2019                            12   \n",
            "6                         2019                            12   \n",
            "7                         2019                            12   \n",
            "8                         2019                            12   \n",
            "9                         2019                            12   \n",
            "\n",
            "   1er jour de la semaine_week  \n",
            "0                            1  \n",
            "1                            1  \n",
            "2                            1  \n",
            "3                            1  \n",
            "4                            1  \n",
            "5                            1  \n",
            "6                            1  \n",
            "7                            1  \n",
            "8                            1  \n",
            "9                            1  \n",
            "\n",
            "============================================================\n",
            "üìã DATA QUALITY VALIDATION: Master Regional Dataset\n",
            "============================================================\n",
            "‚úÖ No duplicate rows\n",
            "‚úÖ Taux de passages aux urgences pour grippe: No negative values\n",
            "‚úÖ Taux d'hospitalisations apr√®s passages aux urgences pour grippe: No negative values\n",
            "‚úÖ Taux d'actes m√©dicaux SOS m√©decins pour grippe: No negative values\n",
            "‚úÖ Date range: 2019-12-30 to 2025-10-06 (2107 days)\n",
            "üìä Regions found: 18\n",
            "\n",
            "============================================================\n",
            "‚úÖ All data quality checks passed!\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüîó CREATING UNIFIED DATASET\\n\" + \"=\"*60)\n",
        "\n",
        "# Start with emergency data (has the best weekly time series)\n",
        "if df_emergency is not None:\n",
        "\n",
        "    # Find potential date and region columns based on keywords\n",
        "    potential_date_cols = [c for c in df_emergency.columns if 'date' in c.lower() or 'semaine' in c.lower() or 'week' in c.lower() or 'jour' in c.lower()]\n",
        "    potential_region_cols = [c for c in df_emergency.columns if any(kw in c.lower() for kw in ['region', 'r√©gion', 'territoire']) and 'code' not in c.lower()]\n",
        "\n",
        "    print(f\"Potential date columns found: {potential_date_cols}\")\n",
        "    print(f\"Potential region columns found: {potential_region_cols}\")\n",
        "\n",
        "    # Select the most likely date and region columns - be more robust\n",
        "    date_col = None\n",
        "    if '1er jour de la semaine' in potential_date_cols:\n",
        "        date_col = '1er jour de la semaine'\n",
        "    elif 'Date' in potential_date_cols:\n",
        "        date_col = 'Date'\n",
        "    elif 'date' in potential_date_cols:\n",
        "        date_col = 'date'\n",
        "    elif potential_date_cols:\n",
        "        # Fallback to the first column found if specific one not present\n",
        "        date_col = potential_date_cols[0]\n",
        "\n",
        "    region_col = None\n",
        "    if 'R√©gion' in potential_region_cols:\n",
        "        region_col = 'R√©gion'\n",
        "    elif 'Region' in potential_region_cols:\n",
        "        region_col = 'Region'\n",
        "    elif 'region' in potential_region_cols:\n",
        "        region_col = 'region'\n",
        "    elif potential_region_cols:\n",
        "        # Fallback to the first column found\n",
        "        region_col = potential_region_cols[0]\n",
        "\n",
        "\n",
        "    if not date_col:\n",
        "        print(\"‚ùå Could not find a suitable date column in emergency data.\")\n",
        "        df_master = None\n",
        "    elif not region_col:\n",
        "         print(\"‚ùå Could not find a suitable region column in emergency data.\")\n",
        "         df_master = None\n",
        "    else:\n",
        "        print(f\"üìÖ Using date column: {date_col}\")\n",
        "        print(f\"üó∫Ô∏è Using region column: {region_col}\")\n",
        "\n",
        "        # Create base dataset\n",
        "        df_master = df_emergency.copy()\n",
        "\n",
        "        # Rename for clarity\n",
        "        df_master = df_master.rename(columns={\n",
        "            date_col: 'date',\n",
        "            region_col: 'region'\n",
        "        })\n",
        "\n",
        "        # Ensure date is datetime\n",
        "        df_master['date'] = pd.to_datetime(df_master['date'], errors='coerce', dayfirst=True)\n",
        "\n",
        "        # Remove rows with missing date or region\n",
        "        initial_rows = len(df_master)\n",
        "        df_master = df_master.dropna(subset=['date', 'region'])\n",
        "        rows_dropped = initial_rows - len(df_master)\n",
        "        \n",
        "        if rows_dropped > 0:\n",
        "            print(f\"   ‚ö†Ô∏è Dropped {rows_dropped} rows with missing date or region\")\n",
        "\n",
        "        # Clean region names (remove extra spaces, standardize case)\n",
        "        df_master['region'] = df_master['region'].str.strip()\n",
        "\n",
        "        # Sort by date and region\n",
        "        df_master = df_master.sort_values(['date', 'region']).reset_index(drop=True)\n",
        "        \n",
        "        # Validate data quality\n",
        "        print(f\"\\n‚úÖ Base dataset created: {df_master.shape}\")\n",
        "        print(f\"üìÖ Date range: {df_master['date'].min()} to {df_master['date'].max()}\")\n",
        "        print(f\"üó∫Ô∏è Regions: {df_master['region'].nunique()} unique regions\")\n",
        "        print(f\"   Regions: {sorted(df_master['region'].unique().tolist())}\")\n",
        "        print(f\"üìä Total weeks: {df_master['date'].nunique()}\")\n",
        "        \n",
        "        # Check for data completeness\n",
        "        expected_rows = df_master['region'].nunique() * df_master['date'].nunique()\n",
        "        completeness = (len(df_master) / expected_rows) * 100\n",
        "        print(f\"üìä Data completeness: {completeness:.1f}%\")\n",
        "\n",
        "        # Show sample\n",
        "        print(f\"\\nüëÄ Sample of unified dataset:\")\n",
        "        print(df_master.head(10))\n",
        "        \n",
        "        # Comprehensive data quality validation\n",
        "        quality_issues = validate_data_quality(df_master, \"Master Regional Dataset\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå Cannot create unified dataset - emergency data missing\")\n",
        "    df_master = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5494ffb6",
      "metadata": {
        "id": "5494ffb6"
      },
      "source": [
        "---\n",
        "\n",
        "## üíæ Save Cleaned Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f4fea796",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4fea796",
        "outputId": "b2586326-0be8-4f61-fd23-65debdc1173b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ SAVING CLEANED DATA\n",
            "============================================================\n",
            "‚úÖ Saved: vaccination_coverage_regional_clean.csv\n",
            "‚úÖ Saved: emergency_visits_regional_clean.csv\n",
            "‚úÖ Saved: master_dataset_regional.csv\n",
            "‚úÖ Saved: master_dataset_regional.pkl\n",
            "‚úÖ Saved: flu_campaign_2021-2022_campagne_clean.csv\n",
            "‚úÖ Saved: flu_campaign_2021-2022_couverture_clean.csv\n",
            "‚úÖ Saved: flu_campaign_2021-2022_doses-actes_clean.csv\n",
            "‚úÖ Saved: flu_campaign_2022-2023_campagne_clean.csv\n",
            "‚úÖ Saved: flu_campaign_2022-2023_couverture_clean.csv\n",
            "‚úÖ Saved: flu_campaign_2022-2023_doses-actes_clean.csv\n",
            "‚úÖ Saved: flu_campaign_2023-2024_campagne_clean.csv\n",
            "‚úÖ Saved: flu_campaign_2023-2024_couverture_clean.csv\n",
            "‚úÖ Saved: flu_campaign_2023-2024_doses-actes_clean.csv\n",
            "‚úÖ Saved: flu_campaign_2024-2025_campagne_clean.csv\n",
            "‚úÖ Saved: flu_campaign_2024-2025_couverture_clean.csv\n",
            "‚úÖ Saved: flu_campaign_2024-2025_doses-actes_clean.csv\n",
            "\n",
            "‚úÖ All cleaned data saved to: c:\\Users\\gabin\\Desktop\\epitech\\hackaton-sante\\projet\\data\\processed\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüíæ SAVING CLEANED DATA\\n\" + \"=\"*60)\n",
        "\n",
        "# Save individual cleaned datasets\n",
        "if df_vaccination is not None:\n",
        "    vax_output = OUTPUT_PATH / 'vaccination_coverage_regional_clean.csv'\n",
        "    df_vaccination.to_csv(vax_output, index=False, encoding='utf-8-sig')\n",
        "    print(f\"‚úÖ Saved: {vax_output.name}\")\n",
        "\n",
        "if df_emergency is not None:\n",
        "    emerg_output = OUTPUT_PATH / 'emergency_visits_regional_clean.csv'\n",
        "    df_emergency.to_csv(emerg_output, index=False, encoding='utf-8-sig')\n",
        "    print(f\"‚úÖ Saved: {emerg_output.name}\")\n",
        "\n",
        "# Save unified master dataset\n",
        "if df_master is not None:\n",
        "    master_output = OUTPUT_PATH / 'master_dataset_regional.csv'\n",
        "    df_master.to_csv(master_output, index=False, encoding='utf-8-sig')\n",
        "    print(f\"‚úÖ Saved: {master_output.name}\")\n",
        "\n",
        "    # Also save as pickle for faster loading\n",
        "    pickle_output = OUTPUT_PATH / 'master_dataset_regional.pkl'\n",
        "    df_master.to_pickle(pickle_output)\n",
        "    print(f\"‚úÖ Saved: {pickle_output.name}\")\n",
        "\n",
        "# Save flu campaigns\n",
        "for year, year_data in flu_campaigns.items():\n",
        "    for file_type, df in year_data.items():\n",
        "        filename = f'flu_campaign_{year}_{file_type}_clean.csv'\n",
        "        output_file = OUTPUT_PATH / filename\n",
        "        df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
        "        print(f\"‚úÖ Saved: {filename}\")\n",
        "\n",
        "print(f\"\\n‚úÖ All cleaned data saved to: {OUTPUT_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa0be50c",
      "metadata": {
        "id": "fa0be50c"
      },
      "source": [
        "---\n",
        "\n",
        "## üìù Data Quality Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b93de1d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b93de1d9",
        "outputId": "742e5509-8f83-4216-accf-5c6bb9327154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìù DATA QUALITY REPORT\n",
            "============================================================\n",
            "{\n",
            "  \"cleaning_date\": \"2025-10-21T15:06:23.851446\",\n",
            "  \"datasets_processed\": [\n",
            "    {\n",
            "      \"name\": \"vaccination_coverage\",\n",
            "      \"rows\": 238,\n",
            "      \"columns\": 19,\n",
            "      \"regions\": \"N/A\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"emergency_visits\",\n",
            "      \"rows\": 27180,\n",
            "      \"columns\": 11,\n",
            "      \"regions\": \"N/A\"\n",
            "    }\n",
            "  ],\n",
            "  \"master_dataset\": {\n",
            "    \"rows\": 27180,\n",
            "    \"columns\": 11,\n",
            "    \"regions\": 18,\n",
            "    \"date_range\": \"2019-12-30 00:00:00 to 2025-10-06 00:00:00\",\n",
            "    \"weeks\": 302,\n",
            "    \"completeness\": \"88.3%\"\n",
            "  }\n",
            "}\n",
            "\n",
            "‚úÖ Report saved to: c:\\Users\\gabin\\Desktop\\epitech\\hackaton-sante\\projet\\data\\processed\\data_quality_report.json\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüìù DATA QUALITY REPORT\\n\" + \"=\"*60)\n",
        "\n",
        "report = {\n",
        "    'cleaning_date': datetime.now().isoformat(),\n",
        "    'datasets_processed': [],\n",
        "    'master_dataset': {}\n",
        "}\n",
        "\n",
        "if df_vaccination is not None:\n",
        "    report['datasets_processed'].append({\n",
        "        'name': 'vaccination_coverage',\n",
        "        'rows': len(df_vaccination),\n",
        "        'columns': len(df_vaccination.columns),\n",
        "        'regions': df_vaccination['region'].nunique() if 'region' in df_vaccination.columns else 'N/A'\n",
        "    })\n",
        "\n",
        "if df_emergency is not None:\n",
        "    report['datasets_processed'].append({\n",
        "        'name': 'emergency_visits',\n",
        "        'rows': len(df_emergency),\n",
        "        'columns': len(df_emergency.columns),\n",
        "        'regions': df_emergency['region'].nunique() if 'region' in df_emergency.columns else 'N/A'\n",
        "    })\n",
        "\n",
        "if df_master is not None:\n",
        "    report['master_dataset'] = {\n",
        "        'rows': len(df_master),\n",
        "        'columns': len(df_master.columns),\n",
        "        'regions': int(df_master['region'].nunique()),\n",
        "        'date_range': f\"{df_master['date'].min()} to {df_master['date'].max()}\",\n",
        "        'weeks': int(df_master['date'].nunique()),\n",
        "        'completeness': f\"{(1 - df_master.isnull().sum().sum() / (df_master.shape[0] * df_master.shape[1])) * 100:.1f}%\"\n",
        "    }\n",
        "\n",
        "# Save report\n",
        "import json\n",
        "report_path = OUTPUT_PATH / 'data_quality_report.json'\n",
        "with open(report_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(report, f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "print(json.dumps(report, indent=2, default=str))\n",
        "print(f\"\\n‚úÖ Report saved to: {report_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "757b6690",
      "metadata": {
        "id": "757b6690"
      },
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "**What we accomplished**:\n",
        "1. ‚úÖ Loaded all data sources with proper encoding\n",
        "2. ‚úÖ Standardized date formats\n",
        "3. ‚úÖ Cleaned region names\n",
        "4. ‚úÖ Created unified regional dataset\n",
        "5. ‚úÖ Saved clean data for next steps\n",
        "\n",
        "**Next Steps**:\n",
        "- üìä **02_Exploratory_Analysis.ipynb**: Visualize patterns and trends\n",
        "- üîÆ **03_Forecasting.ipynb**: Build predictive models\n",
        "- üéØ **04_Optimization.ipynb**: Optimize vaccine distribution\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
