{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfec639f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-10-22T10:12:07.466545Z",
     "iopub.status.busy": "2025-10-22T10:12:07.466067Z",
     "iopub.status.idle": "2025-10-22T10:12:07.951445Z",
     "shell.execute_reply": "2025-10-22T10:12:07.950859Z"
    },
    "id": "cfec639f",
    "outputId": "aa1df66a-47df-4a8a-b1b1-d9bcaf098b53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Libraries loaded\n",
      " 2025-10-22 12:12\n",
      " Environment: Local\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Detect environment (check if running in Google Colab)\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Mount Google Drive if in Colab\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\" Google Drive mounted\")\n",
    "\n",
    "print(\" Libraries loaded\")\n",
    "print(f\" {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\" Environment: {'Google Colab' if IN_COLAB else 'Local'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae922bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-10-22T10:12:07.953048Z",
     "iopub.status.busy": "2025-10-22T10:12:07.952855Z",
     "iopub.status.idle": "2025-10-22T10:12:07.956248Z",
     "shell.execute_reply": "2025-10-22T10:12:07.955619Z"
    },
    "id": "7ae922bc",
    "outputId": "27a9cf98-e008-434b-9cf2-faef158d65dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input: C:\\Users\\gabin\\Desktop\\epitech\\hackaton-sante\\projet\\DATASET\n",
      " Output: C:\\Users\\gabin\\Desktop\\epitech\\hackaton-sante\\projet\\data\\processed\n",
      " Data exists: True\n"
     ]
    }
   ],
   "source": [
    "# Paths (works both locally and in Colab)\n",
    "if IN_COLAB:\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/HACKATHON_DATALAB')\n",
    "else:\n",
    "    BASE_PATH = Path.cwd()\n",
    "\n",
    "DATA_PATH = BASE_PATH / 'DATASET'\n",
    "OUTPUT_PATH = BASE_PATH / 'data' / 'processed'\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\" Input: {DATA_PATH}\")\n",
    "print(f\" Output: {OUTPUT_PATH}\")\n",
    "print(f\" Data exists: {DATA_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c0d3ce",
   "metadata": {
    "id": "41c0d3ce"
   },
   "source": [
    "---\n",
    "\n",
    "## ðŸ› ï¸ Helper Functions\n",
    "\n",
    "These are the workhorses that do the actual cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe31d98b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-10-22T10:12:07.958009Z",
     "iopub.status.busy": "2025-10-22T10:12:07.957690Z",
     "iopub.status.idle": "2025-10-22T10:12:07.971685Z",
     "shell.execute_reply": "2025-10-22T10:12:07.970898Z"
    },
    "id": "fe31d98b",
    "outputId": "b68aaac0-d814-46d2-9b7e-7db7cef38f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Helper functions defined (with enhanced validation)\n"
     ]
    }
   ],
   "source": [
    "def load_csv_robust(filepath, encodings=['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']):\n",
    "    \"\"\"\n",
    "    Load CSV with multiple encoding attempts.\n",
    "    French data is notorious for encoding issues.\n",
    "    \"\"\"\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(filepath, encoding=encoding, low_memory=False)\n",
    "            print(f\"Loaded with {encoding}: {filepath.name}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    print(f\" Could not load: {filepath.name}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_dates_flexible(df, date_columns=None):\n",
    "    \"\"\"\n",
    "    Try to parse date columns with multiple formats.\n",
    "    French dates can be: dd/mm/yyyy, yyyy-mm-dd, or text like 'Semaine 2024-01'\n",
    "    \"\"\"\n",
    "    if date_columns is None:\n",
    "        # Auto-detect date columns\n",
    "        date_keywords = ['date', 'semaine', 'week', 'annee', 'year', 'periode', 'jour']\n",
    "        date_columns = [col for col in df.columns if any(kw in col.lower() for kw in date_keywords)]\n",
    "\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                # Try standard parsing first\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)\n",
    "\n",
    "                # If most values parsed successfully, keep it\n",
    "                if df[col].notna().sum() / len(df) > 0.5:\n",
    "                    print(f\"    Parsed date column: {col}\")\n",
    "                    # Create year and month for aggregation\n",
    "                    df[f'{col}_year'] = df[col].dt.year\n",
    "                    df[f'{col}_month'] = df[col].dt.month\n",
    "                    df[f'{col}_week'] = df[col].dt.isocalendar().week\n",
    "            except Exception as e:\n",
    "                print(f\"    Could not parse {col}: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def standardize_region_names(df, region_col=None):\n",
    "    \"\"\"\n",
    "    Clean up region names to match official 13 French regions.\n",
    "    \"\"\"\n",
    "    # Official 13 French regions (post-2016 reform)\n",
    "    official_regions = [\n",
    "        'Auvergne-RhÃ´ne-Alpes',\n",
    "        'Bourgogne-Franche-ComtÃ©',\n",
    "        'Bretagne',\n",
    "        'Centre-Val de Loire',\n",
    "        'Corse',\n",
    "        'Grand Est',\n",
    "        'Hauts-de-France',\n",
    "        'ÃŽle-de-France',\n",
    "        'Normandie',\n",
    "        'Nouvelle-Aquitaine',\n",
    "        'Occitanie',\n",
    "        'Pays de la Loire',\n",
    "        \"Provence-Alpes-CÃ´te d'Azur\"\n",
    "    ]\n",
    "\n",
    "    # Auto-detect region column\n",
    "    if region_col is None:\n",
    "        region_keywords = ['region', 'rÃ©gion', 'territoire']\n",
    "        for col in df.columns:\n",
    "            if any(kw in col.lower() for kw in region_keywords) and 'code' not in col.lower():\n",
    "                region_col = col\n",
    "                break\n",
    "\n",
    "    if region_col and region_col in df.columns:\n",
    "        # Clean up common variations\n",
    "        df[region_col] = df[region_col].astype(str).str.strip()\n",
    "\n",
    "        # Common replacements\n",
    "        replacements = {\n",
    "            'Ile-de-France': 'ÃŽle-de-France',\n",
    "            'Ile de France': 'ÃŽle-de-France',\n",
    "            'PACA': \"Provence-Alpes-CÃ´te d'Azur\",\n",
    "            'Provence-Alpes-Cote d\\'Azur': \"Provence-Alpes-CÃ´te d'Azur\",\n",
    "            'Auvergne-Rhone-Alpes': 'Auvergne-RhÃ´ne-Alpes',\n",
    "            'Bourgogne-Franche-Comte': 'Bourgogne-Franche-ComtÃ©'\n",
    "        }\n",
    "\n",
    "        df[region_col] = df[region_col].replace(replacements)\n",
    "\n",
    "        # Show unique regions found\n",
    "        unique_regions = df[region_col].unique()\n",
    "        print(f\"    Found {len(unique_regions)} unique regions\")\n",
    "\n",
    "        # Check for unmapped regions\n",
    "        unmapped = set(unique_regions) - set(official_regions) - {'nan', 'None', ''}\n",
    "        if unmapped:\n",
    "            print(f\"    Unmapped regions: {unmapped}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_missing_values(df, strategy='report'):\n",
    "    \"\"\"\n",
    "    Handle missing values intelligently with comprehensive reporting.\n",
    "    Strategy:\n",
    "    - 'report': Just show what's missing\n",
    "    - 'drop': Drop rows with any missing values\n",
    "    - 'fill': Fill with appropriate defaults\n",
    "    \"\"\"\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "\n",
    "    if missing.any():\n",
    "        print(f\"\\n    Missing values:\")\n",
    "        for col in missing[missing > 0].index:\n",
    "            print(f\"      {col}: {missing[col]:,} ({missing_pct[col]:.1f}%)\")\n",
    "\n",
    "        if strategy == 'drop':\n",
    "            original_len = len(df)\n",
    "            df = df.dropna()\n",
    "            print(f\"    Dropped {original_len - len(df):,} rows with missing values\")\n",
    "\n",
    "        elif strategy == 'fill':\n",
    "            # Fill numeric columns with 0, object columns with 'Unknown'\n",
    "            for col in df.columns:\n",
    "                if df[col].dtype in ['float64', 'int64']:\n",
    "                    df[col].fillna(0, inplace=True)\n",
    "                else:\n",
    "                    df[col].fillna('Unknown', inplace=True)\n",
    "            print(f\"    Filled missing values\")\n",
    "    else:\n",
    "        print(f\"    No missing values\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def validate_data_quality(df, name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Comprehensive data quality validation checks.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" DATA QUALITY VALIDATION: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # 1. Check for duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\" Found {duplicates} duplicate rows ({duplicates/len(df)*100:.1f}%)\")\n",
    "        issues.append(f\"Duplicates: {duplicates}\")\n",
    "    else:\n",
    "        print(f\" No duplicate rows\")\n",
    "    \n",
    "    # 2. Check numeric columns for negative values where inappropriate\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    for col in numeric_cols:\n",
    "        if 'taux' in col.lower() or 'nombre' in col.lower() or 'passage' in col.lower():\n",
    "            negative_count = (df[col] < 0).sum()\n",
    "            if negative_count > 0:\n",
    "                print(f\" {col}: {negative_count} negative values (should be positive)\")\n",
    "                issues.append(f\"{col}: negative values\")\n",
    "            else:\n",
    "                print(f\" {col}: No negative values\")\n",
    "    \n",
    "    # 3. Check for outliers (values > 3 standard deviations)\n",
    "    for col in numeric_cols:\n",
    "        if df[col].std() > 0:  # Avoid division by zero\n",
    "            mean = df[col].mean()\n",
    "            std = df[col].std()\n",
    "            outliers = ((df[col] < mean - 3*std) | (df[col] > mean + 3*std)).sum()\n",
    "            if outliers > 0:\n",
    "                pct = outliers / len(df) * 100\n",
    "                if pct > 5:  # Only warn if > 5% are outliers\n",
    "                    print(f\" {col}: {outliers} outliers ({pct:.1f}%) - may need investigation\")\n",
    "                    issues.append(f\"{col}: {outliers} outliers\")\n",
    "    \n",
    "    # 4. Check date column consistency\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df['date']\n",
    "        if pd.api.types.is_datetime64_any_dtype(date_col):\n",
    "            # Check for future dates\n",
    "            today = pd.Timestamp.now()\n",
    "            future_dates = (date_col > today).sum()\n",
    "            if future_dates > 0:\n",
    "                print(f\" Date column: {future_dates} future dates (suspicious)\")\n",
    "                issues.append(f\"Future dates: {future_dates}\")\n",
    "            \n",
    "            # Check for very old dates (before 2010)\n",
    "            very_old = (date_col < pd.Timestamp('2010-01-01')).sum()\n",
    "            if very_old > 0:\n",
    "                print(f\" Date column: {very_old} dates before 2010 (check if valid)\")\n",
    "                issues.append(f\"Very old dates: {very_old}\")\n",
    "            \n",
    "            # Check date range\n",
    "            date_range = (date_col.max() - date_col.min()).days\n",
    "            print(f\" Date range: {date_col.min().date()} to {date_col.max().date()} ({date_range} days)\")\n",
    "    \n",
    "    # 5. Check regional coverage\n",
    "    if 'region' in df.columns:\n",
    "        unique_regions = df['region'].nunique()\n",
    "        expected_regions = 13  # France has 13 regions\n",
    "        print(f\" Regions found: {unique_regions}\")\n",
    "        if unique_regions < expected_regions:\n",
    "            print(f\" Expected {expected_regions} regions, found {unique_regions}\")\n",
    "            issues.append(f\"Missing regions: {expected_regions - unique_regions}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    if issues:\n",
    "        print(f\" Found {len(issues)} data quality issues:\")\n",
    "        for issue in issues:\n",
    "            print(f\"   - {issue}\")\n",
    "        print(f\"\\n Review these issues before proceeding with analysis\")\n",
    "    else:\n",
    "        print(f\" All data quality checks passed!\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "print(\" Helper functions defined (with enhanced validation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41e0813a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-10-22T10:12:07.973485Z",
     "iopub.status.busy": "2025-10-22T10:12:07.973335Z",
     "iopub.status.idle": "2025-10-22T10:12:07.984303Z",
     "shell.execute_reply": "2025-10-22T10:12:07.983730Z"
    },
    "id": "41e0813a",
    "outputId": "5e4f881b-79d8-4b81-b418-cb5455c8ded9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LOADING VACCINATION COVERAGE DATA\n",
      "============================================================\n",
      "Loaded with utf-8: couvertures-vaccinales-des-adolescents-et-adultes-depuis-2011-region.csv\n",
      "\n",
      " Original shape: (238, 19)\n",
      " Columns: ['AnnÃ©e', 'RÃ©gion Code', 'RÃ©gion', 'HPV filles 1 dose Ã  15 ans', 'HPV filles 2 doses Ã  16 ans', 'HPV garÃ§ons 1 dose Ã  15 ans', 'HPV garÃ§ons 2 doses Ã  16 ans', 'MÃ©ningocoque C 10-14 ans', 'MÃ©ningocoque C 15-19 ans', 'MÃ©ningocoque C 20-24 ans', 'Grippe moins de 65 ans Ã  risque', 'Grippe 65 ans et plus', 'Grippe 65-74 ans', 'Grippe 75 ans et plus', 'Covid-19 65 ans et plus', 'Grippe rÃ©sidents en Ehpad', 'Grippe professionnels en Ehpad', 'Covid-19 rÃ©sidents en Ehpad', 'Covid-19 professionnels en Ehpad']\n",
      "    Found 17 unique regions\n",
      "    Unmapped regions: {'Martinique', 'Bourgogne et Franche-ComtÃ©', 'Nouvelle Aquitaine', 'Guyane', 'RÃ©union', 'Auvergne et RhÃ´ne-Alpes', 'Guadeloupe'}\n",
      "\n",
      "    Missing values:\n",
      "      HPV filles 1 dose Ã  15 ans: 20 (8.4%)\n",
      "      HPV filles 2 doses Ã  16 ans: 20 (8.4%)\n",
      "      HPV garÃ§ons 1 dose Ã  15 ans: 187 (78.6%)\n",
      "      HPV garÃ§ons 2 doses Ã  16 ans: 187 (78.6%)\n",
      "      MÃ©ningocoque C 10-14 ans: 59 (24.8%)\n",
      "      MÃ©ningocoque C 15-19 ans: 59 (24.8%)\n",
      "      MÃ©ningocoque C 20-24 ans: 221 (92.9%)\n",
      "      Grippe moins de 65 ans Ã  risque: 97 (40.8%)\n",
      "      Grippe 65 ans et plus: 97 (40.8%)\n",
      "      Grippe 65-74 ans: 190 (79.8%)\n",
      "      Grippe 75 ans et plus: 190 (79.8%)\n",
      "      Covid-19 65 ans et plus: 221 (92.9%)\n",
      "      Grippe rÃ©sidents en Ehpad: 238 (100.0%)\n",
      "      Grippe professionnels en Ehpad: 238 (100.0%)\n",
      "      Covid-19 rÃ©sidents en Ehpad: 238 (100.0%)\n",
      "      Covid-19 professionnels en Ehpad: 238 (100.0%)\n",
      "\n",
      " Cleaned vaccination data: (238, 19)\n",
      "\n",
      " Sample:\n",
      "   AnnÃ©e  RÃ©gion Code               RÃ©gion  HPV filles 1 dose Ã  15 ans  \\\n",
      "0   2011           24  Centre-Val de Loire                        32.0   \n",
      "1   2011           44            Grand Est                        35.6   \n",
      "2   2011           32      Hauts-de-France                        38.7   \n",
      "\n",
      "   HPV filles 2 doses Ã  16 ans  HPV garÃ§ons 1 dose Ã  15 ans  \\\n",
      "0                         28.6                          NaN   \n",
      "1                         30.0                          NaN   \n",
      "2                         35.2                          NaN   \n",
      "\n",
      "   HPV garÃ§ons 2 doses Ã  16 ans  MÃ©ningocoque C 10-14 ans  \\\n",
      "0                           NaN                       NaN   \n",
      "1                           NaN                       NaN   \n",
      "2                           NaN                       NaN   \n",
      "\n",
      "   MÃ©ningocoque C 15-19 ans  MÃ©ningocoque C 20-24 ans  \\\n",
      "0                       NaN                       NaN   \n",
      "1                       NaN                       NaN   \n",
      "2                       NaN                       NaN   \n",
      "\n",
      "   Grippe moins de 65 ans Ã  risque  Grippe 65 ans et plus  Grippe 65-74 ans  \\\n",
      "0                              NaN                    NaN               NaN   \n",
      "1                              NaN                    NaN               NaN   \n",
      "2                              NaN                    NaN               NaN   \n",
      "\n",
      "   Grippe 75 ans et plus  Covid-19 65 ans et plus  Grippe rÃ©sidents en Ehpad  \\\n",
      "0                    NaN                      NaN                        NaN   \n",
      "1                    NaN                      NaN                        NaN   \n",
      "2                    NaN                      NaN                        NaN   \n",
      "\n",
      "   Grippe professionnels en Ehpad  Covid-19 rÃ©sidents en Ehpad  \\\n",
      "0                             NaN                          NaN   \n",
      "1                             NaN                          NaN   \n",
      "2                             NaN                          NaN   \n",
      "\n",
      "   Covid-19 professionnels en Ehpad  \n",
      "0                               NaN  \n",
      "1                               NaN  \n",
      "2                               NaN  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n LOADING VACCINATION COVERAGE DATA\\n\" + \"=\"*60)\n",
    "\n",
    "# Regional vaccination coverage (our focus)\n",
    "vax_path = DATA_PATH / 'Couvertures-vaccinales-des-adolescents-et-adultes' / 'DonnÃ©es-rÃ©gionales' / 'couvertures-vaccinales-des-adolescents-et-adultes-depuis-2011-region.csv'\n",
    "\n",
    "if vax_path.exists():\n",
    "    df_vaccination = load_csv_robust(vax_path)\n",
    "\n",
    "    if df_vaccination is not None:\n",
    "        print(f\"\\n Original shape: {df_vaccination.shape}\")\n",
    "        print(f\" Columns: {list(df_vaccination.columns)}\")\n",
    "\n",
    "        # Parse dates\n",
    "        df_vaccination = parse_dates_flexible(df_vaccination)\n",
    "\n",
    "        # Standardize regions\n",
    "        df_vaccination = standardize_region_names(df_vaccination)\n",
    "\n",
    "        # Check missing values\n",
    "        df_vaccination = handle_missing_values(df_vaccination, strategy='report')\n",
    "\n",
    "        print(f\"\\n Cleaned vaccination data: {df_vaccination.shape}\")\n",
    "        print(f\"\\n Sample:\")\n",
    "        print(df_vaccination.head(3))\n",
    "else:\n",
    "    print(f\" File not found: {vax_path}\")\n",
    "    df_vaccination = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c7396a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-10-22T10:12:07.986055Z",
     "iopub.status.busy": "2025-10-22T10:12:07.985901Z",
     "iopub.status.idle": "2025-10-22T10:12:08.044102Z",
     "shell.execute_reply": "2025-10-22T10:12:08.042447Z"
    },
    "id": "14c7396a",
    "outputId": "074d5bb7-9647-470b-ca0d-c49227e45d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LOADING EMERGENCY ROOM DATA\n",
      "============================================================\n",
      "Loaded with utf-8: grippe-passages-urgences-et-actes-sos-medecin_reg.csv\n",
      "\n",
      " Original shape: (27180, 8)\n",
      " Columns: ['1er jour de la semaine', 'Semaine', 'RÃ©gion Code', 'RÃ©gion', \"Classe d'Ã¢ge\", 'Taux de passages aux urgences pour grippe', \"Taux d'hospitalisations aprÃ¨s passages aux urgences pour grippe\", \"Taux d'actes mÃ©dicaux SOS mÃ©decins pour grippe\"]\n",
      "    Parsed date column: 1er jour de la semaine\n",
      "    Found 18 unique regions\n",
      "    Unmapped regions: {'Martinique', 'Bourgogne et Franche-ComtÃ©', 'Nouvelle Aquitaine', 'Mayotte', 'Guyane', 'RÃ©union', 'Auvergne et RhÃ´ne-Alpes', 'Guadeloupe'}\n",
      "\n",
      "    Missing values:\n",
      "      Semaine: 27,180 (100.0%)\n",
      "      Taux de passages aux urgences pour grippe: 890 (3.3%)\n",
      "      Taux d'hospitalisations aprÃ¨s passages aux urgences pour grippe: 897 (3.3%)\n",
      "      Taux d'actes mÃ©dicaux SOS mÃ©decins pour grippe: 6,045 (22.2%)\n",
      "\n",
      " Cleaned emergency data: (27180, 11)\n",
      "\n",
      " Sample:\n",
      "  1er jour de la semaine Semaine  RÃ©gion Code   RÃ©gion    Classe d'Ã¢ge  \\\n",
      "0             2023-02-20     NaT            6  Mayotte       00-04 ans   \n",
      "1             2023-02-20     NaT            6  Mayotte       15-64 ans   \n",
      "2             2023-02-20     NaT            6  Mayotte  65 ans ou plus   \n",
      "\n",
      "   Taux de passages aux urgences pour grippe  \\\n",
      "0                                 383.141762   \n",
      "1                                 728.597450   \n",
      "2                                2127.659574   \n",
      "\n",
      "   Taux d'hospitalisations aprÃ¨s passages aux urgences pour grippe  \\\n",
      "0                                             1562.5                 \n",
      "1                                                0.0                 \n",
      "2                                                0.0                 \n",
      "\n",
      "   Taux d'actes mÃ©dicaux SOS mÃ©decins pour grippe  \\\n",
      "0                                             NaN   \n",
      "1                                             NaN   \n",
      "2                                             NaN   \n",
      "\n",
      "   1er jour de la semaine_year  1er jour de la semaine_month  \\\n",
      "0                         2023                             2   \n",
      "1                         2023                             2   \n",
      "2                         2023                             2   \n",
      "\n",
      "   1er jour de la semaine_week  \n",
      "0                            8  \n",
      "1                            8  \n",
      "2                            8  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n LOADING EMERGENCY ROOM DATA\\n\" + \"=\"*60)\n",
    "\n",
    "# Regional emergency data (weekly time series)\n",
    "emerg_path = DATA_PATH / 'Passages-aux-urgences-et-Actes-SOS-MÃ©decins' / 'DonnÃ©es-rÃ©gionales' / 'grippe-passages-urgences-et-actes-sos-medecin_reg.csv'\n",
    "\n",
    "if emerg_path.exists():\n",
    "    df_emergency = load_csv_robust(emerg_path)\n",
    "\n",
    "    if df_emergency is not None:\n",
    "        print(f\"\\n Original shape: {df_emergency.shape}\")\n",
    "        print(f\" Columns: {list(df_emergency.columns)}\")\n",
    "\n",
    "        # Parse dates\n",
    "        df_emergency = parse_dates_flexible(df_emergency)\n",
    "\n",
    "        # Standardize regions\n",
    "        df_emergency = standardize_region_names(df_emergency)\n",
    "\n",
    "        # Check missing values\n",
    "        df_emergency = handle_missing_values(df_emergency, strategy='report')\n",
    "\n",
    "        print(f\"\\n Cleaned emergency data: {df_emergency.shape}\")\n",
    "        print(f\"\\n Sample:\")\n",
    "        print(df_emergency.head(3))\n",
    "else:\n",
    "    print(f\" File not found: {emerg_path}\")\n",
    "    df_emergency = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72891f2a",
   "metadata": {
    "id": "72891f2a"
   },
   "source": [
    "---\n",
    "\n",
    "## ðŸ’‰ Load & Clean: Flu Campaign Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73fa6e7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-10-22T10:12:08.046776Z",
     "iopub.status.busy": "2025-10-22T10:12:08.046446Z",
     "iopub.status.idle": "2025-10-22T10:12:08.092257Z",
     "shell.execute_reply": "2025-10-22T10:12:08.091477Z"
    },
    "id": "73fa6e7b",
    "outputId": "5337c402-f0f6-42e2-c5da-4e70f0f2a134"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LOADING FLU CAMPAIGN DATA\n",
      "============================================================\n",
      "\n",
      " Processing 2021-2022...\n",
      "Loaded with utf-8: campagne-2021.csv\n",
      "    Parsed date column: date\n",
      "   campagne: (5, 8)\n",
      "Loaded with utf-8: couverture-2021.csv\n",
      "    Found 13 unique regions\n",
      "    Unmapped regions: {'94 - CORSE', '84 - AUVERGNE-RHONE-ALPES', '27 - BOURGOGNE-FRANCHE-COMTE', '75 - NOUVELLE-AQUITAINE', '76 - OCCITANIE', '32 - HAUTS-DE-France', \"93 - PROVENCE-ALPES-COTES-D'AZUR\", '53 - BRETAGNE', '28 - NORMANDIE', '11 - ILE-DE-France', '44 - GRAND-EST', '52 - PAYS-DE-LA-LOIRE', '24 - CENTRE-VAL-DE-LOIRE'}\n",
      "   couverture: (52, 5)\n",
      "Loaded with utf-8: doses-actes-2021.csv\n",
      "    Parsed date column: date\n",
      "    Parsed date column: jour\n",
      "   doses-actes: (1076, 12)\n",
      "\n",
      " Processing 2022-2023...\n",
      "Loaded with utf-8: campagne-2022.csv\n",
      "    Parsed date column: date\n",
      "   campagne: (5, 8)\n",
      "Loaded with utf-8: couverture-2022.csv\n",
      "    Found 13 unique regions\n",
      "    Unmapped regions: {'94 - CORSE', '84 - AUVERGNE-RHONE-ALPES', '27 - BOURGOGNE-FRANCHE-COMTE', '75 - NOUVELLE-AQUITAINE', '76 - OCCITANIE', '32 - HAUTS-DE-France', \"93 - PROVENCE-ALPES-COTES-D'AZUR\", '53 - BRETAGNE', '28 - NORMANDIE', '11 - ILE-DE-France', '44 - GRAND-EST', '52 - PAYS-DE-LA-LOIRE', '24 - CENTRE-VAL-DE-LOIRE'}\n",
      "   couverture: (52, 5)\n",
      "Loaded with utf-8: doses-actes-2022.csv\n",
      "    Parsed date column: date\n",
      "    Parsed date column: jour\n",
      "   doses-actes: (1056, 12)\n",
      "\n",
      " Processing 2023-2024...\n",
      "Loaded with utf-8: campagne-2023.csv\n",
      "    Parsed date column: date\n",
      "   campagne: (5, 8)\n",
      "Loaded with utf-8: couverture-2023.csv\n",
      "    Found 13 unique regions\n",
      "    Unmapped regions: {'94 - CORSE', '84 - AUVERGNE-RHONE-ALPES', '27 - BOURGOGNE-FRANCHE-COMTE', '75 - NOUVELLE-AQUITAINE', '76 - OCCITANIE', '32 - HAUTS-DE-France', \"93 - PROVENCE-ALPES-COTES-D'AZUR\", '53 - BRETAGNE', '28 - NORMANDIE', '11 - ILE-DE-France', '44 - GRAND-EST', '52 - PAYS-DE-LA-LOIRE', '24 - CENTRE-VAL-DE-LOIRE'}\n",
      "   couverture: (52, 5)\n",
      "Loaded with utf-8: doses-actes-2023.csv\n",
      "    Parsed date column: date\n",
      "    Parsed date column: jour\n",
      "   doses-actes: (1076, 12)\n",
      "\n",
      " Processing 2024-2025...\n",
      "Loaded with utf-8: campagne-2024.csv\n",
      "    Parsed date column: date\n",
      "   campagne: (5, 8)\n",
      "Loaded with utf-8: couverture-2024.csv\n",
      "    Found 13 unique regions\n",
      "    Unmapped regions: {'94 - CORSE', '84 - AUVERGNE-RHONE-ALPES', '27 - BOURGOGNE-FRANCHE-COMTE', '75 - NOUVELLE-AQUITAINE', '76 - OCCITANIE', '32 - HAUTS-DE-France', \"93 - PROVENCE-ALPES-COTES-D'AZUR\", '53 - BRETAGNE', '28 - NORMANDIE', '11 - ILE-DE-France', '44 - GRAND-EST', '52 - PAYS-DE-LA-LOIRE', '24 - CENTRE-VAL-DE-LOIRE'}\n",
      "   couverture: (52, 5)\n",
      "Loaded with utf-8: doses-actes-2024.csv\n",
      "    Parsed date column: date\n",
      "    Parsed date column: jour\n",
      "   doses-actes: (964, 12)\n",
      "\n",
      " Loaded 4 campaign years\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n LOADING FLU CAMPAIGN DATA\\n\" + \"=\"*60)\n",
    "\n",
    "# Load all campaign years\n",
    "flu_campaigns = {}\n",
    "flu_base = DATA_PATH / 'Vaccination-Grippe'\n",
    "\n",
    "for year_folder in sorted(flu_base.glob('Vaccination-Grippe-*')):\n",
    "    year = year_folder.name.replace('Vaccination-Grippe-', '')\n",
    "    print(f\"\\n Processing {year}...\")\n",
    "\n",
    "    year_data = {}\n",
    "\n",
    "    for csv_file in sorted(year_folder.glob('*.csv')):\n",
    "        file_type = csv_file.stem.rsplit('-', 1)[0]  # Get 'campagne', 'couverture', 'doses-actes'\n",
    "\n",
    "        df = load_csv_robust(csv_file)\n",
    "        if df is not None:\n",
    "            df = parse_dates_flexible(df)\n",
    "            df = standardize_region_names(df)\n",
    "            year_data[file_type] = df\n",
    "            print(f\"   {file_type}: {df.shape}\")\n",
    "\n",
    "    if year_data:\n",
    "        flu_campaigns[year] = year_data\n",
    "\n",
    "print(f\"\\n Loaded {len(flu_campaigns)} campaign years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "236c6ca4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-10-22T10:12:08.094202Z",
     "iopub.status.busy": "2025-10-22T10:12:08.094003Z",
     "iopub.status.idle": "2025-10-22T10:12:08.137206Z",
     "shell.execute_reply": "2025-10-22T10:12:08.136214Z"
    },
    "id": "236c6ca4",
    "outputId": "1c96d0e5-4b67-4e6d-e7f4-3cfdf2659629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " CREATING UNIFIED DATASET\n",
      "============================================================\n",
      "Potential date columns found: ['1er jour de la semaine', 'Semaine', '1er jour de la semaine_year', '1er jour de la semaine_month', '1er jour de la semaine_week']\n",
      "Potential region columns found: ['RÃ©gion']\n",
      " Using date column: 1er jour de la semaine\n",
      " Using region column: RÃ©gion\n",
      "\n",
      " Base dataset created: (27180, 11)\n",
      " Date range: 2019-12-30 00:00:00 to 2025-10-06 00:00:00\n",
      " Regions: 18 unique regions\n",
      "   Regions: ['Auvergne et RhÃ´ne-Alpes', 'Bourgogne et Franche-ComtÃ©', 'Bretagne', 'Centre-Val de Loire', 'Corse', 'Grand Est', 'Guadeloupe', 'Guyane', 'Hauts-de-France', 'Martinique', 'Mayotte', 'Normandie', 'Nouvelle Aquitaine', 'Occitanie', 'Pays de la Loire', \"Provence-Alpes-CÃ´te d'Azur\", 'RÃ©union', 'ÃŽle-de-France']\n",
      " Total weeks: 302\n",
      " Data completeness: 500.0%\n",
      "\n",
      " Sample of unified dataset:\n",
      "        date Semaine  RÃ©gion Code                      region    Classe d'Ã¢ge  \\\n",
      "0 2019-12-30     NaT           84     Auvergne et RhÃ´ne-Alpes       05-14 ans   \n",
      "1 2019-12-30     NaT           84     Auvergne et RhÃ´ne-Alpes       Tous Ã¢ges   \n",
      "2 2019-12-30     NaT           84     Auvergne et RhÃ´ne-Alpes       00-04 ans   \n",
      "3 2019-12-30     NaT           84     Auvergne et RhÃ´ne-Alpes       15-64 ans   \n",
      "4 2019-12-30     NaT           84     Auvergne et RhÃ´ne-Alpes  65 ans ou plus   \n",
      "5 2019-12-30     NaT           27  Bourgogne et Franche-ComtÃ©       15-64 ans   \n",
      "6 2019-12-30     NaT           27  Bourgogne et Franche-ComtÃ©  65 ans ou plus   \n",
      "7 2019-12-30     NaT           27  Bourgogne et Franche-ComtÃ©       00-04 ans   \n",
      "8 2019-12-30     NaT           27  Bourgogne et Franche-ComtÃ©       05-14 ans   \n",
      "9 2019-12-30     NaT           27  Bourgogne et Franche-ComtÃ©       Tous Ã¢ges   \n",
      "\n",
      "   Taux de passages aux urgences pour grippe  \\\n",
      "0                                 650.142219   \n",
      "1                                 526.218269   \n",
      "2                                 784.199826   \n",
      "3                                 519.702651   \n",
      "4                                 340.193911   \n",
      "5                                 706.531105   \n",
      "6                                 100.738751   \n",
      "7                                 926.705981   \n",
      "8                                 495.662949   \n",
      "9                                 547.512992   \n",
      "\n",
      "   Taux d'hospitalisations aprÃ¨s passages aux urgences pour grippe  \\\n",
      "0                                           0.000000                 \n",
      "1                                         308.344575                 \n",
      "2                                         621.118012                 \n",
      "3                                          51.867220                 \n",
      "4                                         471.512770                 \n",
      "5                                           0.000000                 \n",
      "6                                          76.628352                 \n",
      "7                                        1369.863014                 \n",
      "8                                           0.000000                 \n",
      "9                                         126.262626                 \n",
      "\n",
      "   Taux d'actes mÃ©dicaux SOS mÃ©decins pour grippe  \\\n",
      "0                                     2995.008320   \n",
      "1                                     3097.696585   \n",
      "2                                      564.334086   \n",
      "3                                     4415.440156   \n",
      "4                                     1000.834028   \n",
      "5                                     6311.360449   \n",
      "6                                     2564.102564   \n",
      "7                                     2727.272727   \n",
      "8                                     5220.883534   \n",
      "9                                     5079.100749   \n",
      "\n",
      "   1er jour de la semaine_year  1er jour de la semaine_month  \\\n",
      "0                         2019                            12   \n",
      "1                         2019                            12   \n",
      "2                         2019                            12   \n",
      "3                         2019                            12   \n",
      "4                         2019                            12   \n",
      "5                         2019                            12   \n",
      "6                         2019                            12   \n",
      "7                         2019                            12   \n",
      "8                         2019                            12   \n",
      "9                         2019                            12   \n",
      "\n",
      "   1er jour de la semaine_week  \n",
      "0                            1  \n",
      "1                            1  \n",
      "2                            1  \n",
      "3                            1  \n",
      "4                            1  \n",
      "5                            1  \n",
      "6                            1  \n",
      "7                            1  \n",
      "8                            1  \n",
      "9                            1  \n",
      "\n",
      "============================================================\n",
      " DATA QUALITY VALIDATION: Master Regional Dataset\n",
      "============================================================\n",
      " No duplicate rows\n",
      " Taux de passages aux urgences pour grippe: No negative values\n",
      " Taux d'hospitalisations aprÃ¨s passages aux urgences pour grippe: No negative values\n",
      " Taux d'actes mÃ©dicaux SOS mÃ©decins pour grippe: No negative values\n",
      " Date range: 2019-12-30 to 2025-10-06 (2107 days)\n",
      " Regions found: 18\n",
      "\n",
      "============================================================\n",
      " All data quality checks passed!\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n CREATING UNIFIED DATASET\\n\" + \"=\"*60)\n",
    "\n",
    "# Start with emergency data (has the best weekly time series)\n",
    "if df_emergency is not None:\n",
    "\n",
    "    # Find potential date and region columns based on keywords\n",
    "    potential_date_cols = [c for c in df_emergency.columns if 'date' in c.lower() or 'semaine' in c.lower() or 'week' in c.lower() or 'jour' in c.lower()]\n",
    "    potential_region_cols = [c for c in df_emergency.columns if any(kw in c.lower() for kw in ['region', 'rÃ©gion', 'territoire']) and 'code' not in c.lower()]\n",
    "\n",
    "    print(f\"Potential date columns found: {potential_date_cols}\")\n",
    "    print(f\"Potential region columns found: {potential_region_cols}\")\n",
    "\n",
    "    # Select the most likely date and region columns - be more robust\n",
    "    date_col = None\n",
    "    if '1er jour de la semaine' in potential_date_cols:\n",
    "        date_col = '1er jour de la semaine'\n",
    "    elif 'Date' in potential_date_cols:\n",
    "        date_col = 'Date'\n",
    "    elif 'date' in potential_date_cols:\n",
    "        date_col = 'date'\n",
    "    elif potential_date_cols:\n",
    "        # Fallback to the first column found if specific one not present\n",
    "        date_col = potential_date_cols[0]\n",
    "\n",
    "    region_col = None\n",
    "    if 'RÃ©gion' in potential_region_cols:\n",
    "        region_col = 'RÃ©gion'\n",
    "    elif 'Region' in potential_region_cols:\n",
    "        region_col = 'Region'\n",
    "    elif 'region' in potential_region_cols:\n",
    "        region_col = 'region'\n",
    "    elif potential_region_cols:\n",
    "        # Fallback to the first column found\n",
    "        region_col = potential_region_cols[0]\n",
    "\n",
    "\n",
    "    if not date_col:\n",
    "        print(\" Could not find a suitable date column in emergency data.\")\n",
    "        df_master = None\n",
    "    elif not region_col:\n",
    "         print(\" Could not find a suitable region column in emergency data.\")\n",
    "         df_master = None\n",
    "    else:\n",
    "        print(f\" Using date column: {date_col}\")\n",
    "        print(f\" Using region column: {region_col}\")\n",
    "\n",
    "        # Create base dataset\n",
    "        df_master = df_emergency.copy()\n",
    "\n",
    "        # Rename for clarity\n",
    "        df_master = df_master.rename(columns={\n",
    "            date_col: 'date',\n",
    "            region_col: 'region'\n",
    "        })\n",
    "\n",
    "        # Ensure date is datetime\n",
    "        df_master['date'] = pd.to_datetime(df_master['date'], errors='coerce', dayfirst=True)\n",
    "\n",
    "        # Remove rows with missing date or region\n",
    "        initial_rows = len(df_master)\n",
    "        df_master = df_master.dropna(subset=['date', 'region'])\n",
    "        rows_dropped = initial_rows - len(df_master)\n",
    "        \n",
    "        if rows_dropped > 0:\n",
    "            print(f\"    Dropped {rows_dropped} rows with missing date or region\")\n",
    "\n",
    "        # Clean region names (remove extra spaces, standardize case)\n",
    "        df_master['region'] = df_master['region'].str.strip()\n",
    "\n",
    "        # Sort by date and region\n",
    "        df_master = df_master.sort_values(['date', 'region']).reset_index(drop=True)\n",
    "        \n",
    "        # Validate data quality\n",
    "        print(f\"\\n Base dataset created: {df_master.shape}\")\n",
    "        print(f\" Date range: {df_master['date'].min()} to {df_master['date'].max()}\")\n",
    "        print(f\" Regions: {df_master['region'].nunique()} unique regions\")\n",
    "        print(f\"   Regions: {sorted(df_master['region'].unique().tolist())}\")\n",
    "        print(f\" Total weeks: {df_master['date'].nunique()}\")\n",
    "        \n",
    "        # Check for data completeness\n",
    "        expected_rows = df_master['region'].nunique() * df_master['date'].nunique()\n",
    "        completeness = (len(df_master) / expected_rows) * 100\n",
    "        print(f\" Data completeness: {completeness:.1f}%\")\n",
    "\n",
    "        # Show sample\n",
    "        print(f\"\\n Sample of unified dataset:\")\n",
    "        print(df_master.head(10))\n",
    "        \n",
    "        # Comprehensive data quality validation\n",
    "        quality_issues = validate_data_quality(df_master, \"Master Regional Dataset\")\n",
    "\n",
    "else:\n",
    "    print(\" Cannot create unified dataset - emergency data missing\")\n",
    "    df_master = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4fea796",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-10-22T10:12:08.139052Z",
     "iopub.status.busy": "2025-10-22T10:12:08.138834Z",
     "iopub.status.idle": "2025-10-22T10:12:08.333239Z",
     "shell.execute_reply": "2025-10-22T10:12:08.332410Z"
    },
    "id": "f4fea796",
    "outputId": "b2586326-0be8-4f61-fd23-65debdc1173b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SAVING CLEANED DATA\n",
      "============================================================\n",
      " Saved: vaccination_coverage_regional_clean.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: emergency_visits_regional_clean.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: master_dataset_regional.csv"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Saved: master_dataset_regional.pkl\n",
      " Saved: flu_campaign_2021-2022_campagne_clean.csv\n",
      " Saved: flu_campaign_2021-2022_couverture_clean.csv\n",
      " Saved: flu_campaign_2021-2022_doses-actes_clean.csv\n",
      " Saved: flu_campaign_2022-2023_campagne_clean.csv\n",
      " Saved: flu_campaign_2022-2023_couverture_clean.csv\n",
      " Saved: flu_campaign_2022-2023_doses-actes_clean.csv\n",
      " Saved: flu_campaign_2023-2024_campagne_clean.csv\n",
      " Saved: flu_campaign_2023-2024_couverture_clean.csv\n",
      " Saved: flu_campaign_2023-2024_doses-actes_clean.csv\n",
      " Saved: flu_campaign_2024-2025_campagne_clean.csv\n",
      " Saved: flu_campaign_2024-2025_couverture_clean.csv\n",
      " Saved: flu_campaign_2024-2025_doses-actes_clean.csv\n",
      "\n",
      " All cleaned data saved to: C:\\Users\\gabin\\Desktop\\epitech\\hackaton-sante\\projet\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n SAVING CLEANED DATA\\n\" + \"=\"*60)\n",
    "\n",
    "# Save individual cleaned datasets\n",
    "if df_vaccination is not None:\n",
    "    vax_output = OUTPUT_PATH / 'vaccination_coverage_regional_clean.csv'\n",
    "    df_vaccination.to_csv(vax_output, index=False, encoding='utf-8-sig')\n",
    "    print(f\" Saved: {vax_output.name}\")\n",
    "\n",
    "if df_emergency is not None:\n",
    "    emerg_output = OUTPUT_PATH / 'emergency_visits_regional_clean.csv'\n",
    "    df_emergency.to_csv(emerg_output, index=False, encoding='utf-8-sig')\n",
    "    print(f\" Saved: {emerg_output.name}\")\n",
    "\n",
    "# Save unified master dataset\n",
    "if df_master is not None:\n",
    "    master_output = OUTPUT_PATH / 'master_dataset_regional.csv'\n",
    "    df_master.to_csv(master_output, index=False, encoding='utf-8-sig')\n",
    "    print(f\" Saved: {master_output.name}\")\n",
    "\n",
    "    # Also save as pickle for faster loading\n",
    "    pickle_output = OUTPUT_PATH / 'master_dataset_regional.pkl'\n",
    "    df_master.to_pickle(pickle_output)\n",
    "    print(f\" Saved: {pickle_output.name}\")\n",
    "\n",
    "# Save flu campaigns\n",
    "for year, year_data in flu_campaigns.items():\n",
    "    for file_type, df in year_data.items():\n",
    "        filename = f'flu_campaign_{year}_{file_type}_clean.csv'\n",
    "        output_file = OUTPUT_PATH / filename\n",
    "        df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\" Saved: {filename}\")\n",
    "\n",
    "print(f\"\\n All cleaned data saved to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b93de1d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-10-22T10:12:08.335551Z",
     "iopub.status.busy": "2025-10-22T10:12:08.335348Z",
     "iopub.status.idle": "2025-10-22T10:12:08.343547Z",
     "shell.execute_reply": "2025-10-22T10:12:08.342468Z"
    },
    "id": "b93de1d9",
    "outputId": "742e5509-8f83-4216-accf-5c6bb9327154"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DATA QUALITY REPORT\n",
      "============================================================\n",
      "{\n",
      "  \"cleaning_date\": \"2025-10-22T12:12:08.337696\",\n",
      "  \"datasets_processed\": [\n",
      "    {\n",
      "      \"name\": \"vaccination_coverage\",\n",
      "      \"rows\": 238,\n",
      "      \"columns\": 19,\n",
      "      \"regions\": \"N/A\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"emergency_visits\",\n",
      "      \"rows\": 27180,\n",
      "      \"columns\": 11,\n",
      "      \"regions\": \"N/A\"\n",
      "    }\n",
      "  ],\n",
      "  \"master_dataset\": {\n",
      "    \"rows\": 27180,\n",
      "    \"columns\": 11,\n",
      "    \"regions\": 18,\n",
      "    \"date_range\": \"2019-12-30 00:00:00 to 2025-10-06 00:00:00\",\n",
      "    \"weeks\": 302,\n",
      "    \"completeness\": \"88.3%\"\n",
      "  }\n",
      "}\n",
      "\n",
      " Report saved to: C:\\Users\\gabin\\Desktop\\epitech\\hackaton-sante\\projet\\data\\processed\\data_quality_report.json\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n DATA QUALITY REPORT\\n\" + \"=\"*60)\n",
    "\n",
    "report = {\n",
    "    'cleaning_date': datetime.now().isoformat(),\n",
    "    'datasets_processed': [],\n",
    "    'master_dataset': {}\n",
    "}\n",
    "\n",
    "if df_vaccination is not None:\n",
    "    report['datasets_processed'].append({\n",
    "        'name': 'vaccination_coverage',\n",
    "        'rows': len(df_vaccination),\n",
    "        'columns': len(df_vaccination.columns),\n",
    "        'regions': df_vaccination['region'].nunique() if 'region' in df_vaccination.columns else 'N/A'\n",
    "    })\n",
    "\n",
    "if df_emergency is not None:\n",
    "    report['datasets_processed'].append({\n",
    "        'name': 'emergency_visits',\n",
    "        'rows': len(df_emergency),\n",
    "        'columns': len(df_emergency.columns),\n",
    "        'regions': df_emergency['region'].nunique() if 'region' in df_emergency.columns else 'N/A'\n",
    "    })\n",
    "\n",
    "if df_master is not None:\n",
    "    report['master_dataset'] = {\n",
    "        'rows': len(df_master),\n",
    "        'columns': len(df_master.columns),\n",
    "        'regions': int(df_master['region'].nunique()),\n",
    "        'date_range': f\"{df_master['date'].min()} to {df_master['date'].max()}\",\n",
    "        'weeks': int(df_master['date'].nunique()),\n",
    "        'completeness': f\"{(1 - df_master.isnull().sum().sum() / (df_master.shape[0] * df_master.shape[1])) * 100:.1f}%\"\n",
    "    }\n",
    "\n",
    "# Save report\n",
    "import json\n",
    "report_path = OUTPUT_PATH / 'data_quality_report.json'\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(report, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(json.dumps(report, indent=2, default=str))\n",
    "print(f\"\\n Report saved to: {report_path}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
